import os
from dotenv import load_dotenv
import io
import uuid
import math
import time
import urllib.parse
import requests
from typing import List, Dict
from PyPDF2 import PdfReader
from azure.storage.blob import ContainerClient
from openai import AzureOpenAI
from tqdm import tqdm

load_dotenv()
# ================== é…ç½® ==================
# Azure Cognitive Search
SEARCH_ENDPOINT = os.getenv("SEARCH_ENDPOINT")
SEARCH_INDEX    = os.getenv("SEARCH_INDEX")
SEARCH_KEY      = os.getenv("SEARCH_KEY")

# Azure OpenAI Embedding
EMBED_ENDPOINT  = os.getenv("EMBED_ENDPOINT")
EMBED_KEY       = os.getenv("EMBED_KEY")
EMBED_MODEL     = os.getenv("EMBED_MODEL", "text-embedding-3-large")  # å¯é€‰ç¯å¢ƒå˜é‡ï¼Œé»˜è®¤å€¼

# Azure Blob
BLOB_CONN_STR   = os.getenv("BLOB_CONN_STR")
BLOB_CONTAINER  = os.getenv("BLOB_CONTAINER")
BLOB_PREFIX     = os.getenv("BLOB_PREFIX", "")   # å¯é€‰ç¯å¢ƒå˜é‡ï¼Œé»˜è®¤ç©º
BLOB_ACCOUNT    = os.getenv("BLOB_ACCOUNT")      # ç”¨äºæ‹¼ç›´é“¾

# Chunking ç­–ç•¥
CHUNK_SIZE = 1000   # æ¯æ®µå­—ç¬¦æ•°
CHUNK_OVERLAP = 100 # æ®µè½é‡å ï¼Œé¿å…å‰²è£‚
BATCH_UPLOAD = 500  # æ¯æ‰¹æœ€å¤šä¸Šä¼ å¤šå°‘ä¸ª chunk

# ================== å®¢æˆ·ç«¯ ==================
embedding_client = AzureOpenAI(
    api_key=EMBED_KEY, api_version="2024-02-01", azure_endpoint=EMBED_ENDPOINT
)
blob_container = ContainerClient.from_connection_string(BLOB_CONN_STR, BLOB_CONTAINER)

# ================== å·¥å…·å‡½æ•° ==================
def get_embedding(text: str) -> List[float]:
    # ç®€å•çš„é‡è¯•
    for attempt in range(3):
        try:
            resp = embedding_client.embeddings.create(model=EMBED_MODEL, input=text)
            return resp.data[0].embedding
        except Exception as e:
            if attempt == 2:
                raise
            time.sleep(0.6 * (attempt + 1))

def split_text(text: str, size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP) -> List[str]:
    text = (text or "").strip()
    if not text:
        return []
    chunks = []
    start = 0
    n = len(text)
    while start < n:
        end = min(start + size, n)
        chunk = text[start:end]
        chunks.append(chunk)
        if end == n:
            break
        start = end - overlap
        if start < 0:
            start = 0
    return chunks

def build_blob_url(blob_name: str, page_num: int) -> str:
    # å¯¹ blob_name åš URL ç¼–ç ï¼Œé¿å…ç©ºæ ¼
    encoded_name = urllib.parse.quote(blob_name)
    return f"https://{BLOB_ACCOUNT}.blob.core.windows.net/{BLOB_CONTAINER}/{encoded_name}#page={page_num}"

def search_invalid_docs(top: int = 1000, skip: int = 0) -> List[Dict]:
    """æŸ¥æ‰¾ file==null æˆ– file=='' çš„æ–‡æ¡£ï¼ˆåˆ†é¡µï¼‰"""
    url = f"{SEARCH_ENDPOINT}/indexes/{SEARCH_INDEX}/docs/search?api-version=2023-07-01-Preview"
    headers = {"Content-Type": "application/json", "api-key": SEARCH_KEY}
    body = {
        "search": "*",
        "filter": "(file eq null) or (file eq '')",
        "top": top,
        "skip": skip,
        "select": "chunk_id,file,page,url"
    }
    r = requests.post(url, headers=headers, json=body, timeout=30)
    r.raise_for_status()
    return r.json().get("value", [])

def delete_docs_by_chunk_ids(chunk_ids: List[str]) -> None:
    if not chunk_ids:
        return
    url = f"{SEARCH_ENDPOINT}/indexes/{SEARCH_INDEX}/docs/index?api-version=2023-07-01-Preview"
    headers = {"Content-Type": "application/json", "api-key": SEARCH_KEY}
    actions = [{"@search.action": "delete", "chunk_id": cid} for cid in chunk_ids]
    r = requests.post(url, headers=headers, json={"value": actions}, timeout=60)
    r.raise_for_status()

def cleanup_invalid_docs() -> int:
    """å¾ªç¯åˆ†é¡µæ¸…ç†ï¼Œç›´åˆ°æ²¡æœ‰æ— æ•ˆæ–‡æ¡£"""
    total_deleted = 0
    while True:
        # ç®€å•åˆ†é¡µï¼šæ¯æ¬¡å– 1000 æ¡å°è¯•
        batch = search_invalid_docs(top=1000, skip=0)
        if not batch:
            break
        ids = [doc["chunk_id"] for doc in batch if "chunk_id" in doc]
        delete_docs_by_chunk_ids(ids)
        total_deleted += len(ids)
        print(f"Deleted {len(ids)} invalid docs (accumulated: {total_deleted})")
    return total_deleted

def upload_docs_batched(docs: List[Dict], batch_size: int = BATCH_UPLOAD) -> None:
    url = f"{SEARCH_ENDPOINT}/indexes/{SEARCH_INDEX}/docs/index?api-version=2023-07-01-Preview"
    headers = {"Content-Type": "application/json", "api-key": SEARCH_KEY}
    for i in range(0, len(docs), batch_size):
        batch = docs[i:i+batch_size]
        r = requests.post(url, headers=headers, json={"value": batch}, timeout=120)
        try:
            r.raise_for_status()
        except Exception:
            # æ‰“å°å‰ 300 å­—åšæ’æŸ¥
            print("Index upload error:", r.status_code, r.text[:300])
            raise

def process_pdf_blob(blob_name: str) -> List[Dict]:
    """ä» Blob è¯»å– PDFï¼Œåˆ‡åˆ†å¹¶ç”Ÿæˆ chunk æ–‡æ¡£"""
    file_name = os.path.basename(blob_name)
    # æå– folder åç§°ï¼ˆé¡¶çº§ç›®å½•ï¼‰
    folder = os.path.dirname(blob_name).split("/")[0] if "/" in blob_name else ""
    print(f"Processing: {file_name} (folder={folder})")

    stream = io.BytesIO(blob_container.download_blob(blob_name).readall())
    reader = PdfReader(stream)
    docs = []

    for page_idx, page in enumerate(reader.pages, start=1):
        try:
            text = page.extract_text()
        except Exception:
            text = ""
        if not text:
            continue

        for piece in split_text(text):
            vec = get_embedding(piece)
            docs.append({
                "@search.action": "upload",
                "chunk_id": str(uuid.uuid4()),
                "parent_id": file_name,
                "file": file_name,
                "folder": folder,   # ğŸ”¹æ–°å¢å­—æ®µ
                "title": file_name,
                "page": page_idx,
                "url": build_blob_url(blob_name, page_idx),
                "chunk": piece,
                "text_vector": vec
            })
    return docs



def reingest_from_blob(prefix: str = "") -> int:
    """
    éå†æ•´ä¸ªå®¹å™¨ï¼ˆé»˜è®¤ prefix="" è¡¨ç¤ºæ‰€æœ‰ï¼‰ï¼Œ
    å¯¹æ‰€æœ‰ PDF é‡æ–° ingestï¼Œå¹¶æ˜¾ç¤ºè¿›åº¦
    """
    # å…ˆæ•°ä¸€æ•°æ€»å…±æœ‰å¤šå°‘ PDF
    blobs = [b for b in blob_container.list_blobs(name_starts_with=prefix) if b.name.lower().endswith(".pdf")]
    total_pdfs = len(blobs)
    print(f"Found {total_pdfs} PDFs to ingest.\n")

    total_chunks = 0

    # tqdm è¿›åº¦æ¡
    for blob in tqdm(blobs, desc="Ingesting PDFs", unit="pdf"):
        blob_name = blob.name
        docs = process_pdf_blob(blob_name)
        if docs:
            upload_docs_batched(docs)
            total_chunks += len(docs)
            print(f"âœ… {os.path.basename(blob_name)} â†’ {len(docs)} chunks (ç´¯ç§¯: {total_chunks})")
        else:
            print(f"âš ï¸ Skip {blob_name}: no extractable text.")

    print(f"\nâœ… Finished ingesting {total_pdfs} PDFs, total {total_chunks} chunks.")
    return total_chunks


# ================== ä¸»æµç¨‹ ==================
if __name__ == "__main__":
    print("Step 1/2: Cleaning invalid docs (file is null or empty)...")
    deleted = cleanup_invalid_docs()
    print(f"âœ… Cleaned {deleted} docs with empty file field.\n")

    print("Step 2/2: Re-ingesting PDFs from Blob (all folders)...")
    total = reingest_from_blob("")   # ğŸ”¹ä¼ ç©ºå­—ç¬¦ä¸²ï¼Œéå†æ‰€æœ‰ PDF
    print(f"âœ… Reindexed {total} chunks in total.")

